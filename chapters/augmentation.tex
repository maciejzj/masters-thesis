This chapter includes the main part of the work---creation and training of deep neural networks for data augmentation (generation of training images for super-resolution networks).
This process includes preprocessing the training data, deciding on network architectures, implementing models, and conducting the training process.
The chapter ends with a summary of intermediate results (before training the super-resolution network).

\section{Proba-V preprocessing}
As justified in Chapter \ref{ch:scope}, it was decided to use Proba-V as the augmentation training dataset.
As a preprocessing step, before training the high and low-resolution images should be aligned.
Some architectures, like the HighRes-net feature deep learning-based built-in mechanisms for image registration.
However, for training a simple image-downscaling augmentation network, a more traditional approach to image alignment can be implemented.
The Proba-V dataset features a single high-resolution image per many low-resolution ones for the same scene.
The preparation of the dataset requires turning the single high-resolution and multiple low-resolution images into high and low-resolution pairs.
This can be done by multiplying high-resolution images and aligning each copy with the corresponding low-resolution one.

Among the image registration algorithms presented in Section \ref{sec:registration}, phase-correlation was chosen for aligning image pairs in the Proba-V dataset.
This method is suitable for the preprocessing task and is implemented as a part of popular image processing libraries.
Since pictures to be aligned in Proba-V are of different resolutions, they should be resized to a common shape before registration.
It was decided to perform registration in the high-resolution image domain (because during registration images are upscaled anyway to calculate subpixel translations).
Shifting images will create blank columns and rows on their borders.
This problem was minimized by cropping the images.
The low-resolution images were trimmed with a one-pixel border.
High-resolution images are three times larger, consequently, a margin of three pixels was removed from them.
After this step of preprocessing high-res images are $ 378 \times 378 $ pixels and low-resolution photos are $ 126 \times 126 $ pixels.
It is valuable to visualize the effects of the registration process to ensure its correctness.
Because Proba-V images are monochromatic, they can be visualized as different color channels.
This technique is utilized in Figure \ref{fig:proba-registration}.
In this picture, each of the channels---red and blue contains one of the monochromatic images.
If the pictures are aligned perfectly, the image should be yellow because red and blue channels overlay perfectly (red and blue channels mixed in equal proportions give yellow color).
The more unaligned the pictures are, the more red and blue are visible in the picture, which is most visible at distinct edges.
The visualization proves that the phase-correlation-based registration is suitable for the Proba-V dataset---after registration the picture becomes more yellow and the unaligned edges are less visible.
\begin{figure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{proba_unregistered}
        \caption{Proba-V images overlay before registration}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{proba_registered}
        \caption{Proba-V images overlay after registration}
    \end{subfigure}
    \caption{Sample image pair from \textit{Proba-V \gls{nir} train dataset}}
    \label{fig:proba-registration}
\end{figure}

Photos in the Proba-V dataset are saved as 16-bit images; however, only 14-bit values are utilized.
This should be noted to scale the images properly.
Using the Proba-V dataset with real-life high and low-resolution images poses an additional challenge.
As stated, the images contained in the dataset are taken at different moments, thus they differ in exposure and contrast.
Images in high and low-resolution pairs differ in brightness, both at the level of local details and global average values.
Distributions of image exposure in the dataset are presented in Figures \ref{fig:exposure-dist-red} and \ref{fig:exposure-dist-nir} for \gls{red} and \gls{nir} subsets.
\begin{figure}
        \centering
         \begin{tikzpicture}
		 \begin{axis}[width=\textwidth, height=4cm, grid=major, grid style={dashed}, xmin=0, xmax=1, ytick={1, 2}, yticklabels={High-res, Low-res}, xlabel={Image exposure}]
		      \addplot+ [boxplot prepared={draw position=1,
		      							   average=0.30,
		                                   median=0.28,
		                                   lower whisker=0.1,
		                                   upper whisker=0.60,
		                                   upper quartile=0.37, 
		                                   lower quartile=0.21}] 
		                                   coordinates {(1,0.78)(1,0.66)};
		       \addplot+ [boxplot prepared={draw position=2,
		      							   average=0.31,
		                                   median=0.28,
		                                   lower whisker=0.10,
		                                   upper whisker=0.63,
		                                   upper quartile=0.38, 
		                                   lower quartile=0.21}] 
		                                   coordinates {(2,0.83)(2,0.69)};
		  \end{axis}
		  \end{tikzpicture}
    \caption{Exposure (mean-pixel value per image in 14-bit range) distributions in the \gls{red} Proba-V dataset}
    \label{fig:exposure-dist-red}
\end{figure}
\begin{figure}
        \centering
                 \begin{tikzpicture}
		 \begin{axis}[width=\textwidth, height=4cm, grid=major, grid style={dashed}, xmin=0, xmax=1, ytick={1, 2}, yticklabels={High-res, Low-res}, xlabel={Image exposure}]
		      \addplot+ [boxplot prepared={draw position=1,
		      							   average=0.47,
		                                   median=0.44,
		                                   lower whisker=0.31,
		                                   upper whisker=0.65,
		                                   upper quartile=0.52, 
		                                   lower quartile=0.40}] 
		                                   coordinates {(1,0.93)(1,0.85)(1,0.78)};
		       \addplot+ [boxplot prepared={draw position=2,
		      							   average=0.48,
		                                   median=0.45,
		                                   lower whisker=0.27,
		                                   upper whisker=0.68,
		                                   upper quartile=0.54, 
		                                   lower quartile=0.40}]
		                                   coordinates {(2,0.90)(2,0.89)(2,0.82)};
		  \end{axis}
		  \end{tikzpicture}
    \caption{Exposure (mean-pixel value per image in 14-bit range) distributions in the \gls{nir} Proba-V dataset}
    \label{fig:exposure-dist-nir}
\end{figure}

Having a dataset with high and low-resolution images of different brightness and contrast can be an obstacle for learning a neural network.
For this reason, \textit{histogram equalization} \cite{szeliski-2011-cv} was applied to the learning data.
This technique enhances image contrast and evens the histogram of pixel values.
After equalization the cumulative distribution of brightness is close to linear.

As noted in Section \ref{sec:probav}, Proba-V includes binary masks denoting areas of images that are suitable for training.
Some images include noticeable large areas of unusable pixels.
For this reason, nine images with the least percentage of invalid pixels were chosen per scene as training data.
This minimizes the number of incorrect pixels in the training set.

\section{Augmentation network architectures}
The augmentation network is to perform a rather simple task of reducing the size of input pictures.
Contrary to the traditional image downsampling, the networks should learn to shrink images from pairs of real-life images as stated in Chapter \ref{ch:introduction}.
Three architectures that perform this task have been created, they are presented with increasing complexity:
\begin{enumerate}
	\item Simple convolutional network
	\item Encoder-decoder network
	\item \gls{gan}	
\end{enumerate}

\subsection{Simple fully-convolutional network}
The most simplistic approach to shrinking images with deep learning is to build a basic, fully convolutional network.
The most simplistic implementation of this idea uses three convolutional layers with kernels of size $ 3 \times 3 $.
The midmost convolution slides the filter window with a stride of three to decrease the size of input three times.
The last convolutional layer features one filter to reduce the output depth to a single channel.
The \gls{relu} activation function was used for each applicable layer.
A precise description of the architecture is presented in Table \ref{tab:simple-conv-arch}.
\begin{table}
    \centering
    \caption{Simple fully-convolutional network architecture for data augmentation}
    \label{tab:simple-conv-arch}
    \begin{tabular}{ccc}
        \toprule
        Layer type & Output Shape & Number of parameters \\
        \midrule
        $ Input $      & $ 378 \times 378 \times 1 $  & 0                    \\
        $ Conv2D(filters=64, stride=1) $ & $ 378 \times 378 \times 64 $ & 640 \\
        $ ReLu $ & $ 378 \times 378 \times 64 $ & 0 \\
        $ Conv2D(filters=64, stride=3) $ & $ 126 \times 126 \times 64 $ & 36928 \\
        $ ReLU $ & $ 126 \times 126 \times 64 $ & 0 \\
        $ Conv2D(filters=1, stride=1) $ & $ 126 \times 126 \times 1 $ & 577 \\
        $ Sigmoid $ & $ 126 \times 126 \times 1 $ & 0 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Fully-convolutional encoder-decoder network}
The more complex architecture that can be applied is based on the previously mentioned encoder-decoder network scheme.
It is one of the most popular image-to-image neural network architectures.
The encoder shrinks the image by a factor of six instead of three.
Then the image is upscaled two times.
In result, the output image is overall three times smaller; however, it has been processed by more convolutional layers than in the simple convolutional network.

There are various ways to upscale the image during the decoding process.
The main ones are traditional upscaling and a \textit{transposed convolution} (sometimes called \textit{deconvolution}).
The latter performs a convolution and then transposes the output, which makes the spatial dimension grow in size.
However, the transposed convolution layers can produce visible checkerboard artifacts \cite{odena-2016-deconvolution}.
For this reason, the simpler layer architecture based on upscaling was chosen.
Like the simple architecture, the encoder-decoder features a convolution with a single filter at the end.
The detailed description of the network is shown in Table \ref{tab:autoencoder-arch}.
\begin{table}
    \centering
    \caption{Encoder-decoder network architecture for data augmentation}
    \label{tab:autoencoder-arch}
    \begin{tabular}{ccc}
        \toprule
        Layer type & Output Shape & Number of parameters \\
        \midrule
        $ Input $      & $ 378 \times 378 \times 1 $ & 0                    \\
        $ Conv2D(filters=64, stride=1) $ & $ 378 \times 378 \times 64 $ & 640 \\
        $ ReLU $ & $ 378 \times 378 \times 64 $ & 0 \\
        $ Conv2D(filters=64, stride=3) $ & $ 126 \times 126 \times 64 $ & 36928 \\
        $ ReLU $ & $ 126 \times 126 \times 64 $ & 0 \\
        $ Conv2D(filters=64, stride=2) $ & $ 63 \times 63 \times 64 $ & 36928 \\
        $ ReLU $ & $ 63 \times 63 \times 64 $ & 0 \\
        $ UpSampling2D(stride=2) $ & $ 126 \times 126 \times 64 $ & 0 \\
        $ Conv2D(filters=1, stride=1) $ & $ 126 \times 126 \times 1 $ & 577 \\
        $ Sigmoid $ & $ 126 \times 126 \times 1 $ & 0 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Generative Adversarial Network}
The general idea of an adversarial network was covered in Section \ref{sec:gans-overview}.
As mentioned, models with adversarial loss can be used in numerous data generation scenarios.
Usually, the \gls{gan} generator creates output from random values.
However, adversarial networks can also work in a fully-supervised way.
In this work, a \gls{gan} is proposed that transforms high-resolution images to low-resolution, with loss calculated in regard to the discriminator's judgments.
An algorithm for training such a network is provided in the pseudocode as Algorithm \ref{alg:gan-training}.
One further optimization can be applied to \gls{gan}---often better results are achieved if small noise is applied to the labels \cite{sonderby-2016-gannoise}.
\begin{algorithm}
\caption{\gls{gan} training flow (single train step)}
\label{alg:gan-training}
\begin{algorithmic}
	\REQUIRE{$ x $, $ y_{gt} $}
	\STATE $ y_{fake} = generator(x) $
	\STATE $ y_{pred} = discriminator(cat(y_{fake}, y_{gt})) $
	\STATE $ loss_d = lossfn_d(y_{pred}, cat(\mathds{O}, \mathds{1})) $
	\STATE $ optimize_d(loss_d) $
	\STATE $ freeze\_learning(discriminator) $
	\COMMENT{During the generator learning the optimizer should not be trained}
	\STATE $ y_{pred} = discriminator(generator(x)) $
	\STATE $ loss_g = lossfn_g(y_{pred}, \mathds{1}) $
	\COMMENT{Notice the misleading labels matrix}
	\STATE $ optimize_g(loss_g) $
\end{algorithmic}
\end{algorithm}

The final Tensorflow 2 implementation of the custom training loop step is shown in Listing \ref{lst:train_step_gan}.
Since the network is trained in a supervised way each step requires high-resolution input images $ x $ and ground truth low-resolution pictures $ y_{gt} $.

\begin{listing}
\begin{minted}{python}
def train_step(self, data):
        x, y, y_mask = data
        batch_size = tf.shape(x)[0]

        y_fake = self.generator(x)

        # Discriminator training
        discriminator_input = tf.concat([y_fake, y], axis=0)
        fake_labels = tf.zeros((batch_size, 1))
        true_labels = tf.ones((batch_size, 1))
        labels = tf.concat([fake_labels, true_labels], axis=0)
        labels += 0.15 * tf.random.uniform(tf.shape(labels))

        with tf.GradientTape() as tape:
            y_pred = self.discriminator(discriminator_input)
            d_loss = self.loss_fn(labels, y_pred)

        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(
            zip(grads, self.discriminator.trainable_weights))

        # Generator training
        misleading_labels = tf.ones((batch_size, 1))

        with tf.GradientTape() as tape:
            y_pred = self.discriminator(self.generator(x))
            g_loss = self.loss_fn(misleading_labels, y_pred)

        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(
            zip(grads, self.generator.trainable_weights))

        return {'d_loss': d_loss, 'g_loss': g_loss}
\end{minted}
\caption{Custom Tensorflow 2 train step method for training a \gls{gan} network}
\label{lst:train_step_gan}
\end{listing}

The discriminator architecture resembles a common binary classifier with convolutional and pooling layers.
A \textit{leaky \gls{relu}} was used as an activation function.
After applying a series of convolutions, the image is flattened and then densely connected.
A common problem encountered during \gls{gan} networks trainings is the rapid fitting of the discriminator.
Because it is tasked with a much simpler task than the generator, the discriminator tends to overwhelm its opponent.
For this reason, the discriminator is often handicapped in a way.
In this case, it is missing a large densely connected layer after flatten operation, which would be otherwise typically used in a binary classifier.
Moreover, the stride of size two is combined with a pooling operation to perform more rapid spatial shrinking of the image and put the discriminator at a disadvantage.
Furthermore, a dropout layer was used right before the end of the network.
This kind of layer drops a part of data to prevent overfitting (and also slow down the learning process).
A detailed description of the discriminator part of \gls{gan} can be found in Table \ref{tab:discriminator-arch}.
\begin{table}
    \centering
    \caption{Discriminator architecture in the proposed \gls{gan} for data augmentation}
    \label{tab:discriminator-arch}
    \begin{tabular}{ccc}
        \toprule
        Layer type & Output Shape & Number of parameters \\
        \midrule
        $ Input $      & $ 126 \times 126 \times 1 $ & 0                    \\
        $ Conv2D(filters=64, stride=2) $ & $ 63 \times 63 \times 64 $ & 640 \\
        $ LeakyReLU $ & $ 63 \times 63 \times 64 $ & 0 \\
        $ MaxPool2D(stride=2) $ & $ 31 \times 31 \times 64 $ & 0 \\
        $ Conv2D(filters=64, stride=2) $ & $ 16 \times 16 \times 64 $ & 36928 \\
        $ LeakyReLU $ & $ 16 \times 16 \times 64 $ & 0 \\
        $ MaxPool2D(stride=2) $ & $ 8 \times 8 \times 64 $ & 0 \\
        $ Flatten $ & 4096 & 0 \\
        $ Dropout(rate=0.5) $ & 4096 & 0 \\
        $ Dense $ & 1 & 4097 \\
        $ Sigmoid $ & 1 & 0 \\
        \bottomrule
    \end{tabular}
\end{table}
The generator is structured similarly to the previously presented simple fully-convolutional network, as it performs a similar task.
The full architecture is the same as in Table \ref{tab:simple-conv-arch}.

\section{Training details}
The \gls{nir} subset of the Proba-V dataset was used to train the augmentation networks.
As mentioned before, Proba-V contains a set of binary masks designating areas of low-resolution images that may be invalid.
This has been taken into account in the two of the described networks---the simple fully-convolutional network and encoder-decoder.
If masks are used during training, the masked regions of low-res images do not participate in calculating the loss.
This feature works only with pixel-wise losses such as \gls{mae} and \gls{mse}.
Because \gls{ssim} works in a more structural way, it is harder to remove some pixels from the evaluation when using this metric.
For this reason, using masks during loss calculation is not supported when using \gls{ssim}.

Both simple and encoder-decoder networks use \textit{\gls{adam}} \cite{kingma-2014-adam} optimizer for gradient descent.
This kind of optimization algorithm combines the advantages of \textit{momentum} technique and \textit{\gls{rmsprop}} \cite{sun-2019-optimization}.
The momentum component uses a moving average of gradients from consecutive steps to update weights, instead of a single gradient value, as traditional algorithms would use.
This modification helps the gradient descent to avoid slowdowns on plateaus in the search space.
The \gls{rmsprop}-alike part of the \gls{adam} is responsible for adaptive scaling of the learning rate based on the value of squared gradients.
Alongside \gls{adam} optimizer, MSE was used as the loss function during the final training of the two simpler architectures.

To supervise the training process in an unbiased way, a validation data subset should be used.
Validation data is a part of the training set that does not take part in the gradient calculation but is used to calculate loss and metrics every epoch.
This way the ongoing training can be evaluated on the data that is not directly used for the fitting process.
It is important to create the validation subset out of the training data, not the test dataset.
If the test data was used for validation, a data leak from the evaluation step to the training would occur.
In the fitting process of all models, 20\% of the training dataset was used for the validation.
Both of the simpler architectures utilize \textit{early stopping} mechanism.
This technique stops training after a designated number of epochs without an improvement in the validation loss value.
Early stopping helps to avoid overfitting and provides a more rational stop condition than training for an arbitrary number of epochs.
The training histories of the simple convolutional and the encoder-decoder network are presented in Figure \ref{fig:simple-encoder-decoder-train-hist}.
The subfigures show the simultaneous decrease of training and validation loss.
The decreasing loss curves resemble an exponential decay function, which is a sign of a proper training.
\begin{figure}
    \hfill
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
			\begin{axis}[grid=major, grid style={dashed}, width=12cm, legend cell align={left}, height=8cm, xlabel=Epoch, ylabel=Loss]
			\addlegendentry{Simple convolutional}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/simple_conv-dvc-21-07-04-04_47_26-train_epoch_loss.csv};
			\addlegendentry{Encoder-decoder}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/autoencoder-dvc-21-07-04-05_26_47-train_epoch_loss.csv};
			\end{axis}
		\end{tikzpicture}
        \caption{Training loss history}
    \end{subfigure}
    \hfill
    \vskip\baselineskip
    \hfill
	\begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
			\begin{axis}[grid=major, grid style={dashed}, width=12cm, legend cell align={left}, height=8cm, xlabel=Epoch, ylabel=Loss]
			\addlegendentry{Simple convolutional}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/simple_loss-dvc-21-07-04-04_47_26-validation_epoch_loss.csv};
			\addlegendentry{Encoder-decoder}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/autoencoder-dvc-21-07-04-05_26_47-validation_epoch_loss.csv};
			\end{axis}
		\end{tikzpicture}
        \caption{Validation loss history}
    \end{subfigure}
    \hfill
    \caption{Training history of the simple convolutional and encoder-decoder networks}
    \label{fig:simple-encoder-decoder-train-hist}
\end{figure}

Training a \gls{gan} can pose a significant challenge when the stop condition is taken into consideration.
More ordinary networks utilize early stopping mechanism on the validation dataset to limit the number of training epochs, as it was described previously.
When training a \gls{gan}, it is less obvious what should be used as a metric during validation.
The losses calculated during training do not explicitly express the quality of the outcome.
The generator loss indicates how well it performs relative to the discriminator's ability to differentiate real and generated images.
This does not necessarily mean that the generator outputs high-quality images.
It is better to apply a different metric on evaluation that omits the adversarial part of the network.
In this work, validation is based on calculating the \gls{ssim} value of the generator output and ground truth.
This approach is not applicable to all \gls{gan}s because their training scenarios usually lack the possibility of comparing with any reference data.
Having an easily readable validation metric is beneficial for network evaluation; however, in this work, it was not used to utilize early stopping with the \gls{gan} network.
Adversarial networks tend to have more unstable and varied trainings than conventional networks.
Thus, it was decided to train the \gls{gan} with a large fixed number of a hundred epochs.
The history of the training loss of the discriminator and generator parts of the \gls{gan} are presented in Figure \ref{fig:gan-train-hist}.
The model from epoch 94, with the highest \gls{ssim} validation score, was chosen as the best one and used to export augmented dataset in the further course of the work.
\begin{figure}
    \centering
    \begin{tikzpicture}
			\begin{axis}[width=0.75\linewidth, width=12cm, height=8cm, legend cell align={left}, grid=major, grid style={dashed}, xlabel=Epoch, ylabel=Loss]
			\addlegendentry{Discriminator}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/gan-dvc-21-07-04-05_55_06-train-epoch_d_loss.csv};
			\addlegendentry{Generator}
			\addplot+[mark=none] table [x=step, y=value, col sep=comma] {data/gan-dvc-21-07-04-05_55_06-train-epoch_g_loss.csv};
			\end{axis}
		\end{tikzpicture}
    \caption{Training history of \gls{gan} generator and discriminator subnetworks}
    \label{fig:gan-train-hist}
\end{figure}

The contents of the \textit{params.yaml} file used for training the augmentation networks are included in the appendix, in Chapter \ref{ch:appendix-params}.

\section{Intermediate results}
The experiment workflow assumes that an intermediate evaluation step can be performed between training augmentation and super-resolution networks.
The Proba-V test dataset can be used to evaluate how well the low-resolution images are recreated by the neural networks.
It should be noted that the following results are an intermediate step to sanity-check if the augmented pictures resemble the real-life low-resolution ones.
At this step, the results do not guarantee the quality of super-resolution training. 
To outline a fuller picture, the results can be compared with common traditional image-resizing algorithms.
The results are presented in Table \ref{tab:intermediate-results}.
The images generated by the neural networks and most of the interpolation algorithms resemble the real-life low-resolution images with \gls{ssim} with over 0.9.
This indicates that the networks can create images that are fairly similar to real-file images from the Proba-V dataset which is crucial for training super-resolution images in the future.
\begin{sidewaystable}
\centering
\caption{Intermediate results of evaluation on Proba-V test dataset (\gls{ssim} metric, the larger, the better)}
\label{tab:intermediate-results}
\begin{adjustbox}{center}
\rowcolors{1}{}{lightgray!20}
\begin{tabular}{lcccccccc}
\toprule
\gls{ssim}            & Real       & Simple conv & Encoder-decoder & \gls{gan} & Nearest & Bilinear & Bicubic & Lanczos \\
\midrule
Real             &      1.0       &    0.946     & 0.935 &   0.923     &   0.887       & 0.947 & 0.945 & 0.94 \\
Simple conv      & 0.946 & 1.0 &   0.987  &  0.963  & 0.938 & 0.995 & 0.996 & 0.992 \\
Encoder-decoder & 0.935 & 0.987 &  1.0   &     0.958  & 0.91 & 0.982 & 0.982 & 0.978  \\
\gls{gan}              &   0.923          &  0.963             &   0.958  & 1.0 & 0.892 &     0.967    & 0.966 & 0.962 \\
Nearest          & 0.887 & 0.938 &   0.91  & 0.892 & 1.0 & 0.937 & 0.949 & 0.949 \\
Bilinear         & 0.947 & 0.955 &    0.982 & 0.967 & 0.937 & 1.0 & 0.996    &  0.989 \\
Bicubic          & 0.945 & 0.996 &  0.982   & 0.966 & 0.949 & 0.996 &   1.0  & 0.998 \\
Lanczos          & 0.94& 0.992 &    0.978 & 0.962 & 0.949 & 0.989       & 0.998 & 1.0 \\
\bottomrule        
\end{tabular}
\end{adjustbox}
\end{sidewaystable}
The results can also be examined visually by displaying images side-by-side.
Figure \ref{fig:intermediate-results} presents an example of image shrinking using various methods, Figure \ref{fig:intermediate-results-zoomed} presents an enlarged part of the same image.
The visual comparison has been created using the simple convolutional augmentation network.
The perceptible results for other architectures look very similar, for this reason, they are not included in the figure.
\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_hr}
        \caption{Real-life high-resolution image}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_lr}
        \caption{Real-life low-resolution image}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_lr_pred}
        \caption{Low-resolution image created by downsampling the high-resolution one with the augmentation networks}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_lr_bicubic}
        \caption{Low-resolution image created by resizing the high-resolution one with bicubic interpolation}
    \end{subfigure}
    \caption{Intermediate results of evaluation on Proba-V test dataset for the simple convolutional augmentation network}
    \label{fig:intermediate-results}
\end{figure}
\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_zoomed_hr}
        \caption{Real-life high-resolution image}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_zoomed_lr}
        \caption{Real-life low-resolution image}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_zoomed_lr_pred}
        \caption{Low-resolution image created by downsampling the high-resolution one with the augmentation networks}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intermediate_zoomed_lr_bicubic}
        \caption{Low-resolution image created by resizing the high-resolution one with bicubic interpolation}
    \end{subfigure}
    \caption{Zoomed intermediate results of evaluation on Proba-V test dataset for simple convolutional augmentation network}
    \label{fig:intermediate-results-zoomed}
\end{figure}

The intermediate results indicate that the augmentation process works reasonably well and can be used for creating new datasets.
The numeric metrics look fairly similar for all given methods, their usefulness is to be determined during super-resolution training and evaluation.
The visual results also look well, the low-resolution images resemble the high-resolution ones.

\section{Implementation details}
As stated before, all networks were implemented in \textit{Python} using \textit{Tensorflow 2} library.
The built-in Keras interface offers several ways of building models; the most flexible one is called \textit{model subclassing}.
This approach enables using binary masks during training thanks to the possibility to overwrite the fitting method.
A custom train step with masking capability is shown in Listing \ref{lst:train_step_masking}.
\begin{listing}
\begin{minted}{python}
def train_step(self, data):
    x, y, y_mask = data

    with tf.GradientTape() as tape:
        y_pred = self(x, training=True)
        if self._use_lr_masks:
            y_pred = tf.boolean_mask(y_pred, y_mask)
            y = tf.boolean_mask(y, y_mask)
        loss = self.compiled_loss(
            y, y_pred, regularization_losses=self.losses)

    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}
\end{minted}
\caption{Custom train step method with masking capabilites}
\label{lst:train_step_masking}
\end{listing}

Model subclassing involves inheritance to implement custom training loops for Tensorflow models.
The simple convolutional and encoder-decoder networks utilize this kind of model instantiation.
The \gls{gan} model uses a mixture of subclassing and \textit{Sequential API} which defines the model in a declarative way.
The two components of \gls{gan}---the generator and the discriminator are instantiated using the sequential approach, then they are combined into one using model subclassing.
A custom inference preview callback was implemented to enhance the training process.
Every training epoch a preview of inference on validation data is saved to a file.

\textit{Tensorboard} is a package accompanying Tensorflow that provides a convenient interface for training supervision.
It runs as a web server that displays a web page with the live progress of training in the form of interactive plots.
These plots usually contain the history of training and validation loss values throughout the fitting process.
Tensorboard data is created using Keras' callbacks.
Callbacks are \textit{functions hooks} called during specific steps of training.
They can provide features like previously discussed early stopping.
\textit{Model checkpoint} is another useful callback that saves the model at the end of the training epoch.
Tensorflow provides an interface for creating custom callbacks.
An inference preview mechanism on the validation image at the end of each epoch was implemented as a custom callback.
This mechanism is very convenient when training the \gls{gan} network.
Keras also enables writing custom losses and metrics.
This possibility has been used to write training losses based on \gls{psnr} and \gls{ssim}.
Both of them are based on internal Tensorflow implementations that were adjusted to serve as loss functions.